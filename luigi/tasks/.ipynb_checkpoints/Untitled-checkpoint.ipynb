{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-56759690efa7>, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-56759690efa7>\"\u001b[0;36m, line \u001b[0;32m12\u001b[0m\n\u001b[0;31m    def run(self):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import luigi\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "import urllib\n",
    "from tempfile import mktemp\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import requests\n",
    "class DownloadFoodReviewsData(luigi.Task):\n",
    "def run(self):\n",
    "    # URL for the file download.\n",
    "    gzip_file_url = \"https://snap.stanford.edu/data/finefoods.txt.gz\"\n",
    "\n",
    "    # Paths for the data directory, text file, and compressed file.\n",
    "    data_directory = os.path.join('..', 'data')\n",
    "    if not os.path.exists(data_directory):\n",
    "        print(\"Data directory ('{}') does not exist - creating it\".format(data_directory))\n",
    "        os.mkdir(data_directory)\n",
    "\n",
    "    text_file_path = os.path.join(data_directory, 'finefoods.txt')\n",
    "    gzip_file_path = text_file_path + '.gz'\n",
    "\n",
    "    # Function to download a file from a website.\n",
    "    #\n",
    "    # Assumes that the file is large and should be downloaded as a stream.\n",
    "    #\n",
    "    # url - URL to the file\n",
    "    # save_file_path - local path where the file should be saved\n",
    "    # chunk_size - size of the chunks to use when saving the streamed bytes to a file; default = 8KiB\n",
    "    #\n",
    "    # returns - None\n",
    "    def download_data_file(url, save_file_path, chunk_size=8192):\n",
    "        # Request the file as a stream.\n",
    "        response = requests.get(url, stream=True)\n",
    "\n",
    "        # Save the data to the given file_path.\n",
    "        with open(save_file_path, 'wb') as fd:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                fd.write(chunk)\n",
    "\n",
    "        return None\n",
    "    # Function to decompress a gzipped data file.\n",
    "    #\n",
    "    # text_file - path to the decompressed file; will be created\n",
    "    # gzip_file - path to the gzipped file; must already exist\n",
    "    #\n",
    "    # returns - None\n",
    "    def decompress_data_file(text_file, gzip_file):\n",
    "        with gzip.open(gzip_file, 'rb') as infile:\n",
    "            #gz_content = infile.read()\n",
    "            #text_content = str(gz_content)\n",
    "            with open(text_file, 'wb') as outfile:\n",
    "                outfile.write(infile.read())\n",
    "\n",
    "        return None\n",
    "\n",
    "    # Function to get the decompressed data file.\n",
    "    #\n",
    "    # Does nothing if the data file already exists.\n",
    "    # If the data file doesn't exist, but the compressed data file does, then it decompresses the compressed file.\n",
    "    # If the compressed data file doesn't exist, then it downloads and decompresses the data file.\n",
    "    #\n",
    "    # text_file - path to the decompressed file; will be created if it doesn't already exist\n",
    "    # gzip_file - path to the gzipped file; will be downloaded if it doesn't already exist\n",
    "    #\n",
    "    # returns - None\n",
    "    def get_data_file(text_file, gzip_file):\n",
    "        if os.path.exists(text_file):\n",
    "            print(\"Decompressed file ('{}') already exists\".format(text_file))\n",
    "        else:\n",
    "            print(\"Decompressed file ('{}') does not exist\".format(text_file))\t\t\t\n",
    "        # To get the decompressed file, we need the gzipped file.\n",
    "        if not os.path.exists(gzip_file):\n",
    "            print(\"Compressed data file ('{}') does not exist, downloading...\".format(gzip_file))\n",
    "            download_data_file(gzip_file_url, gzip_file)\n",
    "            print(\"...download finished\")\n",
    "\n",
    "            print(\"Compressed data file ('{}') exists, decompressing\".format(gzip_file))\n",
    "            decompress_data_file(text_file, gzip_file)\t\t\t\n",
    "        return None\n",
    "\n",
    "    def output(self):\t\n",
    "        #save file to Data directory\n",
    "        return luigi.LocalTarget('data/finefoods.txt')\t\n",
    "\n",
    "\n",
    "\n",
    "class ConvertFoodReviewsData(luigi.Task):    \n",
    "def requires(self):\n",
    "return DownloadFoodReviewsData()\n",
    "\n",
    "def input(self):\n",
    "return luigi.LocalTarget('../data/finefoods.txt')\n",
    "def run(self):\n",
    "# Paths for the data directory, text file, and compressed file.\n",
    "data_directory = os.path.join('..', 'data')\n",
    "if not os.path.exists(data_directory):\n",
    "    print(\"[ERROR] data directory ('{}') does not exist\".format(data_directory))\n",
    "\n",
    "INPUT_FILE_NAME = \"finefoods.txt\"\n",
    "OUTPUT_FILE_NAME = \"finefoods.csv\"\n",
    "\n",
    "input_filepath = os.path.join(data_directory, INPUT_FILE_NAME)\n",
    "csv_filepath = os.path.join(data_directory, OUTPUT_FILE_NAME)\n",
    "\n",
    "header = [\n",
    "    \"product/productId\",\n",
    "    \"review/userId\",\n",
    "    \"review/profileName\",\n",
    "    \"review/helpfulness\",\n",
    "    \"review/score\",\n",
    "    \"review/time\",\n",
    "    \"review/summary\",\n",
    "    \"review/text\"]\n",
    "\n",
    "simple_header = [\n",
    "    \"productId\",\n",
    "    \"userId\",\n",
    "    \"profileName\",\n",
    "    \"helpfulness\",\n",
    "    \"score\",\n",
    "    \"time\",\n",
    "    \"summary\",\n",
    "    \"text\"]\n",
    "\n",
    "infile = open(input_filepath, \"rt\", encoding=\"Latin-1\")\n",
    "csvfile = open(csv_filepath, \"wt\", encoding=\"UTF-8\")\n",
    "\n",
    "\n",
    "# Write a list of fields as a comma-separated row with a pipe (|) as a quote character.\n",
    "def write_quoted_fields(csvfile, field_list):\n",
    "    csvfile.write('|' + field_list[0] + '|')\n",
    "    for field in field_list[1:]:\n",
    "        csvfile.write(',|' + field + '|')\n",
    "    csvfile.write(\"\\n\")\n",
    "\n",
    "\n",
    "# Write the header line.\n",
    "write_quoted_fields(csvfile, simple_header)\n",
    "\n",
    "# Useful controls during debugging.\n",
    "record_limit = 1000000\n",
    "troublesome_records = [] #370, 211557, 226163, 519217, 521382, 525958, 531539\n",
    "\n",
    "field_count = len(header)\n",
    "line_count = 0\n",
    "record_count = 0\n",
    "currentLine = []\n",
    "for line in infile:\n",
    "    #print(\"Processing line: {}\".format(line.strip()))\n",
    "    line_count += 1\n",
    "    line = line.strip()\n",
    "\n",
    "    if line == \"\":\n",
    "        if len(currentLine) == field_count:\n",
    "            write_quoted_fields(csvfile, currentLine)\n",
    "            record_count += 1\n",
    "\n",
    "            if (record_count+1) in troublesome_records:\n",
    "                print(\"[WARN] troublesome record -1: {}\".format(currentLine))\n",
    "\n",
    "            if record_count in troublesome_records:\n",
    "                print(\"[WARN] troublesome record: {}\".format(currentLine))\n",
    "\n",
    "            if (record_count-1) in troublesome_records:\n",
    "                print(\"[WARN] troublesome record +1: {}\".format(currentLine))\n",
    "\n",
    "        else:\n",
    "            print(\"[WARN] current record appears to be incomplete: {}\".format(currentLine))\n",
    "\n",
    "        currentLine = []\n",
    "        continue\n",
    "\n",
    "    parts = line.split(\": \", 1)\n",
    "\n",
    "    # Check to see if the line looks sensible enough to be added.\n",
    "    if len(parts) == 2:\n",
    "        # If there are pipe characters in the text (unlikely), replace them with a slash.\n",
    "        field = parts[1].strip().replace('|', '/')\n",
    "        currentLine.append(field)\n",
    "    else:\n",
    "        # Throw this away - there are junk lines in the raw file, e.g.:\n",
    "        # review/profileName: Sherry \"Tell us about yourself!\n",
    "        # School Princi...\n",
    "        print(\"[WARN] only found {} parts after splitting line: {}\".format(len(parts), parts))\n",
    "        print(\"[WARN] line was: {}\".format(line))\n",
    "\n",
    "    if record_count > record_limit:\n",
    "        break\n",
    "\n",
    "\n",
    "# Write the final record (if it is complete).\n",
    "if len(currentLine) == field_count:\n",
    "    write_quoted_fields(csvfile, currentLine)\n",
    "    record_count += 1\n",
    "\n",
    "# Close files.\n",
    "infile.close()\n",
    "csvfile.close()\n",
    "\n",
    "print(\"Finished - wrote {} lines and {} records.\".format(line_count, record_count))\n",
    "\n",
    "\n",
    "column_dtypes = {'productId': str, 'userId': str, 'profileName': str, 'helpfulness': str,\n",
    "                 'score': np.float64, 'time': np.int64, 'summary': str, 'text': str}\n",
    "\n",
    "# For this dataset, 'quoting' must be set to QUOTE_ALL (1) and the quotechar to a pipe (|).\n",
    "# The problem is that values in some 'text' fields begin with a \", but don't end with one,\n",
    "# and many review texts contain commas, unbalanced quotes and apostrophes.\n",
    "review_df = pd.read_table(csv_filepath, delimiter=',', encoding=\"UTF-8\", dtype=column_dtypes, \n",
    "                          quoting=1, quotechar='|', engine=\"c\", skip_blank_lines=True, \n",
    "                          error_bad_lines=False, warn_bad_lines=True)\n",
    "# Convert score to an int, since it isn't truly a float.\n",
    "review_df['score'] = review_df['score'].astype(int)\n",
    "\n",
    "review_df['helpfulness_numerator'] = [x[0] for x in review_df['helpfulness'].str.split('/')]\n",
    "review_df['helpfulness_denominator'] = [x[1] for x in review_df['helpfulness'].str.split('/')]\n",
    "review_df['date'] = pd.to_datetime(review_df['time'], unit='s')\n",
    "del review_df['helpfulness']\n",
    "review_df.to_csv(\"data/Reviews.csv\",index=False)\n",
    "def output(self):\n",
    "#save file to Data directory\n",
    "return luigi.LocalTarget('data/Reviews.csv')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "luigi.run()\t\t\t\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
